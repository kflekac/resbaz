{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python for Data Analysis\n",
    "\n",
    "This first session of ResSearch will be focused on the exploration, cleaning and basic visualisation of a prepared set of sample data - the [pedestrian counts](https://github.com/resbaz/pedestriancounts) at various locations around Melbourne. If you have not already downloaded this data, please do so now, and place this data file in the same folder as this jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "Thorughout this session we're going to be teaching you a range of tools and skills related to cleaning, manipulation and visualising large datasets. Using the pandas package, we can read in and manipulate large spreadsheets of data, and matplotlib lets you visualise these datasets in a useable, customisable format.\n",
    "\n",
    "The three sections I'll be taking you through today are:\n",
    "\n",
    "- Data examination\n",
    "- Dataframe manipulation\n",
    "- Asking a research question\n",
    "- Plotting your data with pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "First, we need to import Python's *pandas*, *matplotlib* and *numpy* packages, and then use inline plotting \"magic\" command so that all plots generated will appear within this notebook instead of in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# installing packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline plotting magic command\n",
    "# allows figures to be generated inside the notebook, instead of in a separate window\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "\n",
    "The first step to any data exploration and manipulation is to open your data within your program. We are going to do this using the **pandas** package, which reads in spread-sheet style data and converts them into *dataframes*. \n",
    "\n",
    "These dataframes work with rows and columns, like a spreadsheet, except that all data within a single column has to be the same data type. \n",
    "\n",
    "For example, imagine you had a spreadsheet containing two columns - \"labels\" and \"numbers\", and that the rows in the \"labels\" column contains either a text or number sequence. Because you cannot turn text into a number, every single row in that \"labels\" column would need to be a string (text) type. Similarly, if some (but not all) of the rows in the \"numbers\" column contained decimals, **all** of the rows within this column would need to be of a decimal (float) data type.\n",
    "\n",
    "To read in a comma-separated file, or \\*.csv, you can use the pandas function read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in a *.csv file\n",
    "pdsn = pd.read_csv(\"pedestriancounts_melbourne.csv\",\n",
    "                   # Because the file is missing column names:\n",
    "                   header = None, names = [\"Timestamp\",\"Day\", \"Hour\",\"SensorID\",\"Location\",\"Counts\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also open a variety of other file types using the \"reader\" functions found in this [IO tools documentation](http://pandas.pydata.org/pandas-docs/version/0.20/io.html \"Pandas IO tools\"). \n",
    "\n",
    "This includes file types such as excel (\\*.xlsx) files, and text (\\*.txt) files. You can use the parameters within these functions to specify file or data attributes such as column separators, whether there's column/row names, and even specifying your own column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining your Data\n",
    "\n",
    "- `head()`, `tail()`\n",
    "- `columns`\n",
    "- `df.Column` vs `df['Column']`\n",
    "- `dtypes`\n",
    "- `describe()`\n",
    "- `shape`; `shape[0]` vs. `shape[1]`\n",
    "- `iloc` vs `loc`\n",
    "\n",
    "One of the first steps in exploring your data is to see what it looks like, what data types are present, and how many rows/columns there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `df.head()` and `df.tail()` show you the first/last 5 rows of your dataframe selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shows the first 5 rows of your input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shows the last 5 rows of your input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` function gives you the dimensions of your data, in the form (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a tuple (or linked pairs) of the number of rows and columns in your dataframe.\n",
    "pdsn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the first or second element of the tuple can give you either the rows or the columns\n",
    "#Gives you the rows (remember, 0 indexing!)\n",
    "pdsn.shape[0]\n",
    "\n",
    "#Gives you the columns\n",
    "pdsn.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `df.columns` to examine the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the column names in your dataframe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a particular column in your dataframe, you can use one of two options:\n",
    "\n",
    "1) Calling the column as an \"attribute\"\n",
    "    - `df.columnName`\n",
    "\n",
    "2) Subsetting the dataframe\n",
    "    - `df['columnName']`\n",
    "\n",
    "The first option is useful for some functions, but the second form is essential if you want to call more than column at once. You do this by inserting a list, [], of column names, instead a single column.\n",
    "\n",
    "For example: `df[[\"Column1\", \"Column2\", ... , etc]]`\n",
    "\n",
    "Consider this toy example of a dataframe, s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(\n",
    "                 {'A':['a', 'b', 'c','d','e','f'], \n",
    "                  'B':[54, 67, 89, 100, None, 64],\n",
    "                  'C': np.random.randn(6),\n",
    "                  'D': [2.6,None, 8.0, 9.4, 3.3, None]},\n",
    "                 index=[49, 48, 47, 1, 2, 3])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try subsetting the first two columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you change the order of the columns around?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What about a column that doesnt exist?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "Knowing what type of data is in your dataframe is extremely important, as it limits what functions you can and can't do on that column. It's useless to try and do string manipulations on an integer, or try to find the sum of a column of names.\n",
    "\n",
    "We can quickly view the data in our table by using the `dtypes` function. \n",
    "\n",
    "Within pandas, str = \"object\", int = \"int64\", and float = \"float64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What data types are in each column?\n",
    "pdsn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this, you can now examine a quick summary of your data to see what you're working with with `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdsn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately describe will preferentially only output a summary of any numeric type columns, such as Counts, the SensorID and the Hour. To examine the string columns, you need to use the `include = 'all'` parameter.\n",
    "\n",
    "As all of this data is output in the same table, the rows which aren't applicable to that data type are filled that NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at all of the columns\n",
    "pdsn.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find specific statistics for each of these columns by using `max()`, `min()`, `count()`, `std()`, and `sum()`. \n",
    "\n",
    "Just remember though that many of these functions rely on numeric data types, and will cause errors if used on a str type. Calling `sum()` on a string however will concatentate those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try finding the maximum of the Location (a string) column\n",
    "pdsn.Location.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what about the Standard Deviation (std) of Counts (numeric)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful functions include `mean()` and `unique()`.\n",
    "\n",
    "Where `mean()` takes the average of numeric data, `unique()` works on both numeric and string data, and gives you a list of all of the unique values within a particular column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique works on numeric data\n",
    "pdsn.Hour.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As well as on str data\n",
    "pdsn.Day.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this returns a numpy array though, rather than a pandas dataframe or series object, you can use either len() or .size to find the number of unique values for that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the numpy \"size\" function\n",
    "pdsn.Day.unique().size\n",
    "\n",
    "# Using the len() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataframe slicing\n",
    "Sometimes you need to examine specific rows and columns in the middle of your data though, which aren't covered by `head()` or `tail()`. Instead, you can use `df.iloc['row#','column#']` for position based dataframe navigation, `df.loc[]` for label based navigation, or simple index slicing `df[rowNumbers]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing works similarly to how you might slice a string, or a list. You simply call the indexes of the rows you want from the dataframe:\n",
    "\n",
    "`df[rowNumbers]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.Location[5:10] # Gives rows 5, 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn[[\"Location\",\"Counts\"]][:8] #gives rows 0 through 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `loc` and `iloc` to slice rows and columns.\n",
    "\n",
    "`iloc` is positional based, so only takes integer values that correlate with the row and column numbers you want to subset\n",
    "\n",
    "`loc` is label based, and takes the row and column **labels** as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using iloc to get rows 0, 1 and 2\n",
    "s.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using loc to get row labels 1, 2 and 3\n",
    "s.loc[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only enter one set of values into `loc` and `iloc`, they will return the values for every column in your dataset.\n",
    "\n",
    "By using a second integer though, you can choose which rows and columns you specifically want to subset. The first value corresponds to the row, and the second to the columns, or rows x columns. You can also think of this with the moniker *\"Roman Catholic\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using loc to get row labels 1, 2 and 3 for Columns A and B\n",
    "s.loc[1:3,[\"A\",\"B\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using iloc to get rows 0, 1 and 2 for columns 0 and 1.\n",
    "s.iloc[:3,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between `loc` and `iloc` can seem minor, but they're very important, and which one you should use depends on what your needs are at the time.\n",
    "\n",
    "`iloc` is based on dataframe position, so calling `iloc[:3]` would give you rows 0 through 3. \n",
    "\n",
    "`loc` however is based on the index label, so if you were to call `df.loc[:3]`, it would give you all rows UP TO the row labelled as index 3.\n",
    "\n",
    "While the indexes are in order and all present, this isn't an issue. Consider what happens when the indexes are out of order though, with our dataframe 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see that this only takes the first 2 rows\n",
    "s.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Whereas this takes all rows UP TO index label 2\n",
    "s.loc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `loc` is based on labels, if you try to subset a row or column label that doesn't exist, even if it corresponds to a positional row, python will throw you an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For example, as Python has -indexing, the first row of the dataframe would have position = 0\n",
    "# If we try to subset using this position inside loc however, it will throw an error as there is no row labelled as '0'\n",
    "s.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this, it's important that you carefully consider which tool is appropriate for your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 11th through 20th (inclusive) rows  for the SensorId, Location and Counts columns in pdsn using loc, iloc and slicing\n",
    "#iloc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting with conditionals\n",
    "You can also subset using conditional statements, such as ==, !=, >, <, etc.\n",
    "\n",
    "For example, if I want to find all of the rows in s where B > 60, I would type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns/rows where the value in column B > 60\n",
    "s[s.B > 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with lists and for loops, etc, you can also combine these conditionals using & {and} , or | {or}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the rows where C < 0.5 AND B != 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also get a list of the row indexes for your subset\n",
    "list(s[s.B > 60].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can subset using boolean lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a boolean list. The 1st, 3rd and 5th values are True.\n",
    "na = [True,False,True,False,True,False]\n",
    "\n",
    "s[na] #subsets the 1st, 3rd and 5th rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Challenge 1\n",
    "\n",
    "Find how many rows belong to the sensor at Lygon St (West) between the hours of 12am and 2am?\n",
    "\n",
    "*Hint 1: an entry of \"3\" in the Hour column means that the pedestrian counts have been monitored from 3am to 4am*\n",
    "\n",
    "*Hint 2: remember that you find the length of a list using the \"len()\" function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: how many entries in this dataframe are for the sensor at Lygon St (West) between the hours of 0am and 2am?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Your Dataframe\n",
    "\n",
    "- Finding NaNs: \n",
    "    - `count()` \n",
    "    - Using booleans: `isnull()`\n",
    "    - Chaining: `isnull().any()`, `isnull().sum()`\n",
    "    - Subsetting: `df[df['Column'].isnull()]`\n",
    "- Data type conversion\n",
    "- Using the \"timestamp\" data type\n",
    "- Adding and deleting columns/rows\n",
    "- `groupby()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Null Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we explored the use of `shape` to determine the dimensions of our dataframe. Another way to do this is with `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can see that not all of the columns have the same length. This is because count sums the number of rows within that column that contain values - so missing data, or NaNs, will cause the count to be smaller.\n",
    "\n",
    "You can check which columns inside our test dataframe, s, contains null values by using `isnull()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good thing we didn't do that on our pedestrian dataframe!\n",
    "\n",
    "`isnull()` returns a boolean list for each entry in every column - which is useful for subsetting, but not ideal for a quick summary.\n",
    "\n",
    "By chaining the `isnull()` and `any()` functions together, like `df.isnull().any()`, we can quickly see if a column contains any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls in each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also chain `isnull()` and `sum()` to find an exact count of how many NaNs are in each column as well, like `df.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the number of NaNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolving Null Data\n",
    "\n",
    "There are 2 ways to resolve Null data - replace it with new data, or delete them.\n",
    "\n",
    "For the purposes of this event, we are going to teach you how to delete them (though another section below this will teach you to replace it, for the purposes of your own data investigations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's view the columns where SensorID, Location and Counts have null values.\n",
    "\n",
    "We can do this using the `isnull()` function. As mentioned before, we can subset with a boolean list, which is what `isnull()` returns. Where the test evaluates to 'True', the row is output from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the rows where SensorID has null values\n",
    "pdsn[pdsn['SensorID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the rows where Location has null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Show the rows where Counts has null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also show *every* row with null values at once using the isnull().any() chained operation. any() contains the optional parameter \"axis\", which allows you to choose whether it operates on the rows or the columns. By default axis = 0, which gives you columns, but setting axis = 1 checks over the rows instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn[pdsn.isnull().any(axis = 1)] # nulls in rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of these null values, we need to delete them from each column in separate operations. However, this can easily done by subsetting with the `notnull()` function.\n",
    "\n",
    "`notnull()` performs just as `isnull()` does, and returns a boolean (True/False) list for every entry in that column based on whether that value is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn = pdsn[pdsn.SensorID.notnull()] #removing SensorID nulls\n",
    "pdsn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing Location nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing Counts nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if you have a list of the row indexes you wish to remove, you can also use the `drop` function\n",
    "\n",
    "`drop df.[indexes]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine the `tail()` of the altered dataframe though you can see that the row indexes are still the same from before the deletions though. We can fix this using `reset_index()`.\n",
    "\n",
    "Remember that while the indexes aren't that important if you're using `iloc`, but if you're using `loc` it's important that these index labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdsn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdsn = pdsn.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That ends the \"deleting nulls\" section of data manipulation. To continue with the exercises, now go to \"Converting Data Types, Dataframe Manipulation\". \n",
    "\n",
    "This next text section will show instead how you might replace your null values with useable data.\n",
    "\n",
    "### Replacing Nulls - Personal reading section\n",
    "###### Where one column is dependant on another\n",
    "In the case of SensorID and Location, each of these values are dependent on the other. Where the SensorID is 13, the Location will always be 'Flagstaff Station', etc. In these cases, you can use previous values from your table - where you DON'T have nulls, to replacce your missing data.\n",
    "\n",
    "Where there are only a few points, you might choose to do this manually. This can be done using the `set_value(index,\"Column\", value)` function. For the index parameter, you can pass it either a single index, or a list, [], of indexes to be replaced. \n",
    "\n",
    "For example, if you wanted to replace the singular null value for SensorID at index 243, you would type:\n",
    "\n",
    "`pdsn.set_value(243, \"SensorID\", 9)`\n",
    "\n",
    "Say that you noticed a repeated spelling error in some of the labels, and wanted to fix them, or change something for more clarity. For example, if you wanted to replace all instance of \"Flagstaff Station\" in the Location column with \"Flagstaff\", you might use:\n",
    "\n",
    "`pdsn.set_value(list(pdsn[pdsn.SensorID == 9].index), \"Location\", \"Flagstaff\")`\n",
    "\n",
    "###### Replacing all null values with a single value\n",
    "In the Counts column for example, you can see that there are 14 missing values. You might choose to just replace all of these with 0. You can do this using the `fillna()` function, and specifying the value you want to replace it with.\n",
    "\n",
    "`pdsn['Counts'] = pdsn['Counts'].fillna(value=0)`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a great many other options when replacing nulls, such as using the mean of previous values, or even \"imputing\" your data based on probabalistic models.\n",
    "\n",
    "There are many resources online on how to do these, and I would greatly encourage you to go and explore these. Generally speaking, the best way to find something is to throw as many key words into Google as you can, and something relevant will usually pop up. While it's unlikely that a single question on Stack Overflow will tell you everything you need to know for your specific problem, reading a few related questions will usually help you figure out your issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Types, Dataframe Manipulation\n",
    "\n",
    "While we currently have a \"Timestamp\" column, if you go into dtypes you can see that pandas has read this in as string, rather than as a datetime type. If we left it as a string, to get the Month, Day or Year we would need to perform a series of string manipulations every time we wanted to access these values - a costly, redundant and time consuming exercise if trying to use the whole dataframe. \n",
    "\n",
    "Instead, we can convert this column to a datetime data type, using the `to_datetime()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdsn['Timestamp'] = pd.to_datetime(pdsn['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdsn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a Column with the Day\n",
    "pdsn['Date'] = pd.DatetimeIndex(pdsn['Timestamp']).day\n",
    "\n",
    "# Adding a Column with the Month (from 1-12)\n",
    "\n",
    "\n",
    "# Adding a Column with the Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We accidentally named the 'Year' column with a lowercase. While not necessarily a bad thing, it's good for column names to remain consistent, to reduce confusion and errors while programming.\n",
    "\n",
    "Lets add in another column \"Year\", and get rid of the incorrectly named one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding a new \"Year\" column\n",
    "pdsn['Year'] = pd.DatetimeIndex(pdsn['Timestamp']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Deleting the \"year\" column\n",
    "    # MUST use the df[column] format, df.column will throw an error\n",
    "    \n",
    "del pdsn['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert other columns by using the df[column].astype(<datatype>) function. This includes 'str', 'int64' and 'float64' data types, amongst others.\n",
    "\n",
    "You must be careful **not to use `astype()` on a column with null values**. `astype()` does not preserve the null values, and makes resolving them later more difficult.\n",
    "\n",
    "Let's use our dataframe, s, in an example of converting types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No assignment means that this isn't a permanent change to the dataframe\n",
    "s.C.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 2\n",
    "Convert the D column in s to a string type, and then shown what the resulting values at the previously null indexes are\n",
    "\n",
    "*Hint: you can get the index of the current null values before the conversion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Challenge 2 - Convert the data types and show the new values and their data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to be able to group the data within a column according to the data in another column. \n",
    "\n",
    "For example, you might wish to take the mean pedestrian counts for each month, or each year. We could do this is a complicated and time consuming way where you subset the data for each month, and then take the mean of the Counts column...or we could just use the `groupby` function. `groupby` outputs a reformatted version of your data where all values associated with your \"grouping factor\" are taken together. You can then perform mathematical and statistical tests on this grouped output, and the tests will be performed within each of the \"groups\" you defined.\n",
    "\n",
    "For example, say that we wanted to find the mean pedestrian counts seen in each month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdsn.groupby(by=[\"Month\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately these tests are non-discriminatory, and will take the `mean()` of each numeric data column. \n",
    "\n",
    "To get the columns you want, such as Counts, you need to specify this in the chained command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean pedestrian counts within each month\n",
    "pdsn.groupby(by=[\"Month\"]).mean().Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also specificy multiple columns to be output\n",
    "    # the mean \"Hour\" and pedestrian counts within each month\n",
    "pdsn.groupby(by=[\"Month\"]).mean()[[\"Counts\",\"Hour\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to group over multiple factors, such as by Month and Year together, by specifying a list for the `by` parameter within `groupby()`. The order of the values in this list matters, as `groupby` will first group your data based on the first factor, and THEN the second factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Year, then by month\n",
    "#The average number of pedestrians \n",
    "pdsn.groupby(by=[\"Year\",\"Month\"]).mean().Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the order of the groupby factors matter\n",
    "pdsn.groupby(by=[\"Month\",\"Year\"]).mean().Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chaining more commands together you can also find the maximum, minimum, etc, values for your groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum\n",
    "pdsn.groupby(by=[\"Month\"]).Counts.sum().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But which month is this associated with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Month associated with the minimum value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Challenge\n",
    "Which sensor has the greatest average Pedestrian traffic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optional Challenge Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking a Research Question\n",
    "\n",
    "- Asking your question\n",
    "- Plotting your data in pyplot\n",
    "    - line plots\n",
    "    - bar graphs\n",
    "    - histograms\n",
    "    - scatter plots\n",
    "- Plotting location data on a map (Basemap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned all of our data, and lerned how to view it according to different factors, it's time to think about what kind of questions we might be able to answer with it.\n",
    "\n",
    "A question that we might ask, for example, is \"Does Summer have a greater amount of pedestrian traffic than Winter? Does this change across years?\"\n",
    "\n",
    "To examine this, we would first need to subset the dataframe for the appropriate calendar months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = pdsn.loc[pdsn[\"Month\"].isin([1,2,12,6,7,8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = seasons.reset_index()\n",
    "\n",
    "seasons.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seasons.groupby(by=[\"Year\",\"Month\"]).sum().Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add another column, \"Season\", based on the month column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Column                  If Month in this list        True =   False = \n",
    "seasons[\"Season\"] = np.where(seasons['Month'].isin([1,2,12]), 'Summer', 'Winter')\n",
    "\n",
    "# Check it's been added in\n",
    "seasons.Season.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to check that your values have been assigned properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Summer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Winter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have another column we can group on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a research question to explore, it's time to look at how we might explore that visually.\n",
    "\n",
    "This section will teach you the basics of plotting within matplotlib. This is hardly comprehensive, and there's a wide array of customisability and control with matplotlib and plotly. Feel free to delve into some of user guides and tutorials and questions available on Google and Stack Overflow to learn more about these tools and how to make them work for you.\n",
    "\n",
    "A reasonably good matplotlib.pyplot tutorial can be found at https://matplotlib.org/users/pyplot_tutorial.html\n",
    "\n",
    "The [matplotlib API](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.html) contains all of the functions you can call within matplotlib, along with detailed information about how you can use them and their parameters\n",
    "\n",
    "A handy \"quick guide\" to the different kinds of plotting functions within pyplot can also be found at the [pyplot API](https://matplotlib.org/api/pyplot_api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by trying to plot the seasons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons.plot() #Plots everything on top of each other\n",
    "                 # Not particularly informative, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's not particularly informative, and much of the graph is completely unreadable.\n",
    "\n",
    "Matplotlib.pyplot has great customisability, so let's play around with trying to plot different types of graphs with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Line Plot\n",
    "\n",
    "Line plots are great for plotting series data, usually a time series of some description.\n",
    "\n",
    "Line plots are the default plot drawn when calling `.plot()`.\n",
    "\n",
    "Within the plot function you can also specify the Figure title and the figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot\n",
    "pdsn.groupby(by = ['Year', 'Month']).mean().Counts.plot( # Drawing a pot from the grouped Year, Month data\n",
    "                                                figsize = (8,6), # The figure size\n",
    "                                                title = 'Mean Pedestrian Counts per Month, per Year') #Figure Title\n",
    "\n",
    "#specifying the x-axis label\n",
    "plt.xlabel('Pedestrian Counts per Year, Month')\n",
    "\n",
    "# the y-axis label\n",
    "plt.ylabel('Counts (mean)')\n",
    "\n",
    "#Rotating the x-axis labels\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "#If you want to SAVE your plot, make sure you do it BEFORE you show it in the notebook\n",
    "plt.savefig('PedestrianHistogram.png', dpi=300)\n",
    "\n",
    "#Generating an image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Chart\n",
    "Bar charts are great for categorical data, like examining \"Seasons\", for example.\n",
    "\n",
    "Bar charts aren't generally recommended for time series data, such as plotting counts/hour, or mean counts per month, as these are more suited for line plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You can still use the plot function, but specify which columns and plot type you would like to use\n",
    "seasons.groupby(by = ['Season']).mean().Counts.plot(kind = 'barh',figsize = (8, 6)) \n",
    "\n",
    "#Figure title\n",
    "title(\"Mean Pedestrian Counts per Season\")\n",
    "\n",
    "# x-axis label\n",
    "plt.xlabel('Mean Counts')\n",
    "\n",
    "#y-axis label\n",
    "plt.ylabel('Season')\n",
    "\n",
    "#If you want to SAVE your plot, make sure you do it BEFORE you show it in the notebook\n",
    "plt.savefig('SeasonsBarchart.png', dpi=300)\n",
    "\n",
    "#Generating an image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot\n",
    "Box plots are good for viewing the spread of data within certain categories. These can be useful to measure whether two conditions could be said to be approximately equal, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data and plot to use                                      \n",
    "plt.fig = seasons[[\"Season\", \"Counts\"]].mean().boxplot(by = 'Season', # Separating the data according to season\n",
    "                                                              figsize=(8, 6)) #Specifying the figure size\n",
    "\n",
    "# x-axis label\n",
    "plt.xlabel('Season')\n",
    "\n",
    "#y-axis label\n",
    "plt.ylabel('Mean Counts')\n",
    "\n",
    "#figure title\n",
    "title(\"Pedestrian Counts per Season\")\n",
    "\n",
    "#If you want to SAVE your plot, make sure you do it BEFORE you show it in the notebook\n",
    "#plt.savefig('SeasonsBoxplot.png', dpi=300)\n",
    "\n",
    "#Generating an image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "\n",
    "Histograms let you observe the distribution of your (continuous) data, which means that it isn't suitable for all data types.\n",
    "\n",
    "One thing we could use to observe, however, are how the average number of pedestrians per month is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure of given size\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "#plt.hist takes the data you want to plot\n",
    "plt.hist(pdsn.groupby(by = \"Month\").mean().Counts, bins = 12, normed = False, title = 'Pedestrian Counts per Month')\n",
    "\n",
    "#specifying the x-axis label\n",
    "plt.xlabel('Mean Pedestrian Counts')\n",
    "\n",
    "# the y-axis label\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "#If you want to SAVE your plot, make sure you do it BEFORE you show it in the notebook\n",
    "plt.savefig('PedestrianHistogram.png', dpi=300)\n",
    "\n",
    "#Generating an image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "Scatter plots also allow you observe the relationship between two conditions within your data. This works best when you have two continuous variables. With the use of colours and shapes, you can also observe how certain conditions cluster throughout this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdsn.plot(kind='scatter', x = \"Counts\", y = \"Month\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we don't actually have two continuous variables within this dataset, so our scatter plot sorts according to their discrete categories.\n",
    "\n",
    "Instead, let's try reading in the metadata for the sensor location, in the \"pedestrian_sensor_locations.csv\" file.\n",
    "\n",
    "Tihs file contains latitude and longitude data, and would show a more ideal scatterplot projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensorLt = pd.read_csv(\"pedestrian_sensor_locations.csv\")\n",
    "sensorLt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pdsn.groupby(\"Location\").mean().Counts)/(pdsn.groupby(\"Location\").mean().Counts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a list of \"area\" values, scaled according to the mean pedestrian counts for each location\n",
    "area = (pdsn.groupby(\"Location\").mean().Counts)/(pdsn.groupby(\"Location\").mean().Counts.max()) * 100\n",
    "\n",
    "sensorLt.plot(kind='scatter', x = \"Latitude\", y = \"Longitude\", s = area)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Longitude and Latitude Plotting using Basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plotting UK Median House prices for 2015 onto a map of UK based on postcode:\n",
    "\n",
    "http://www.datadependence.com/2016/06/creating-map-visualisations-in-python/\n",
    "\n",
    "Another tutorial:\n",
    "\n",
    "https://peak5390.wordpress.com/2012/12/08/matplotlib-basemap-tutorial-plotting-points-on-a-simple-map/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the size of the figure\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "\n",
    "# Creating the basic area of the map\n",
    "m = Basemap(resolution='c', # c, l, i, h, f or None - the image resolution, ranging from crude (c) to full (f)\n",
    "            projection='merc', #the type of map used. This is a mercator projection\n",
    "            lat_0=54.5, lon_0=-4.36, # The latitude and the longitude to centre the map on\n",
    "            llcrnrlon=-6., llcrnrlat= 49.5, # The latitude and the longitude of the upper RH corner\n",
    "            urcrnrlon=2., urcrnrlat=55.2) # The latitude and the longitude of the lower LH corner\n",
    "# Worth nothing that the latitude and the longitude must be between -90 and 90.\n",
    "\n",
    "# Setting up the colour of the map\n",
    "m.drawmapboundary(fill_color='#46bcec') \n",
    "m.fillcontinents(color='#f2f2f2',lake_color='#46bcec') #land mass and water colour\n",
    "m.drawcoastlines() # boundary lines\n",
    "\n",
    "#making a function that takes a position and then plots the number of new houses \n",
    "    #associated with that position onto our map represented by the size of the point\n",
    "def plot_area(pos):\n",
    "    count = new_areas.loc[new_areas.pos == pos]['count']\n",
    "    x, y = m(pos[1], pos[0])\n",
    "    size = (count/1000) ** 2 + 3\n",
    "    m.plot(x, y, 'o', markersize=size, color='#444444', alpha=0.8)\n",
    "\n",
    "    #applying this function to the data\n",
    "new_areas.pos.apply(plot_area)\n",
    "\n",
    "\n",
    "#reading in a shapefile of the UK postcode boundaries\n",
    "m.readshapefile('data/uk_postcode_bounds/Areas', 'areas')\n",
    "\n",
    "\n",
    "# Using data to colour in areas \n",
    "\n",
    "# higher the number of new houses in an area, the darker the colour of the area. \n",
    "# Well also add a colour bar in to give people looking at the map an idea of what kind of number a colour represents\n",
    "\n",
    "df_poly = pd.DataFrame({ #Creating a dataframe of these colour values\n",
    "        'shapes': [Polygon(np.array(shape), True) for shape in m.areas], #Using the shapefile previously imported\n",
    "        'area': [area['name'] for area in m.areas_info]\n",
    "    })\n",
    "df_poly = df_poly.merge(new_areas, on='area', how='left')\n",
    "\n",
    "\n",
    "#Actually colouring in the areas \n",
    "cmap = plt.get_cmap('Oranges')   \n",
    "pc = PatchCollection(df_poly.shapes, zorder=2)\n",
    "norm = Normalize()\n",
    " \n",
    "pc.set_facecolor(cmap(norm(df_poly['count'].fillna(0).values)))\n",
    "ax.add_collection(pc)\n",
    "\n",
    "#Adding a colour bar\n",
    "mapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "mapper.set_array(df_poly['count'])\n",
    "plt.colorbar(mapper, shrink=0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Challenge 3\n",
    "\n",
    "Design your own research question, and interrogate and plot your data to investigate your question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This session has taken you through the beginner's guide of how to visualise, clean and investigate your data, and given you the basic tools to answer your own research questions about your data. \n",
    "\n",
    "Python is a very versatile tool, and can go much further than what we've shown you today. Many packages are open source and being developed for a range of purpoess all the time. Scipy allows you to perform basic math and statistical tests for example, there are a host of packages related to investigating biological data.\n",
    "\n",
    "Using a programming language allows you to investigate much larger data than you can do by hand or by eye, and the customisability of the plotting tools makes it ideal for generating useful and professional images  ideal for publication.\n",
    "\n",
    "Follow us on Twitter to keep up-to-date on new and up-coming trainings\n",
    "\n",
    "- [@kflekac](https://twitter.com/kflekac)\n",
    "- [@ResBaz](https://twitter.com/resbaz)\n",
    "- [@ResPlat](https://twitter.com/resplat)\n",
    "\n",
    "There's also a reasonably new facebook group, called \"Data Wrangling with Python\" where you can post your python problems, cool things you've done, and stay apprised of new trainings coming up as well.\n",
    "\n",
    "Also remember that if you're having problems, Research Platforms runs a weekly Hacky Hour, where anybody and everybody is able to enquire about coding and programming problems in a variety of tools. Every Thursday from 3-4pm, at the large table in Tsubu Bar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
